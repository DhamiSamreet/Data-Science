# -*- coding: utf-8 -*-
"""Copy of Assignment2(Data science).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sF2aDlI7t0n0Lkoy63mw-kHCOr7swjmn
"""

from sklearn.datasets import load_iris
data = load_iris()

print(data.data)

print(data.DESCR)

data.feature_names

data.target

data.target_names

import seaborn as sns

import pandas as pd
iris=pd.DataFrame(data.data,columns=data.feature_names)

iris['type']=data.target

iris.head()

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

X = iris.iloc[:, :-1]
#print(X)

y = iris.iloc[:, -1]
x_train ,x_test = train_test_split(iris,test_size=0.2)

sns.boxplot(x="type",y="petal length (cm)",data=x_train)
plt.show()

sns.boxplot(x="type",y="sepal length (cm)",data=x_train)

sns.boxplot(x="type",y="sepal width (cm)",data=x_train)

sns.boxplot(x="type",y="petal width (cm)",data=x_train)

plt.scatter(x="petal length (cm)",y="petal width (cm)",data=x_train)

import numpy as np


counts,bin_edges=np.histogram(x_train['sepal width (cm)'],bins=10,density=True)
pdf=counts/(sum(counts))
print(pdf)
print(bin_edges)

cdf=np.cumsum(pdf)
plt.plot(bin_edges[1:],pdf)
plt.plot(bin_edges[1:],cdf)
plt.show()

# X = feature values, all the columns except the last column
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn import metrics
X = iris.iloc[:, :-1]
print(X)
y = iris.iloc[:, -1]
#print(y)
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=150)
model = LogisticRegression()
model.fit(x_train, y_train)

predictions = model.predict(x_test)

print()# Printing new line

print( classification_report(y_test, predictions) )
print(metrics.confusion_matrix(y_test,predictions))

from sklearn import metrics
from sklearn.naive_bayes import GaussianNB

model = GaussianNB()

model.fit(x_train,y_train)
print(model)

predicted = model.predict(x_test)

print(metrics.classification_report(y_test, predicted))
print(metrics.confusion_matrix(y_test, predicted))

from sklearn.cluster import KMeans
kmalgo = KMeans(n_clusters = 3, n_jobs = 4)
kmalgo.fit(x_train)
y_kmeans = kmalgo.predict(x_test)
print(y_kmeans)
print(metrics.confusion_matrix(y_test, y_kmeans))
print(metrics.accuracy_score(y_test, y_kmeans))

from mpl_toolkits.mplot3d import Axes3D


from sklearn import decomposition

centers = [[1, 1], [-1, -1], [0, 0]]

graph = plt.figure(1, figsize=(5, 4))
plt.clf()
plot = Axes3D(graph, rect=[0, 0, .95, 1])

plt.cla()
pca = decomposition.PCA(n_components=3)
pca.fit(X)
X = pca.transform(X)

for species, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:
    plot.text3D(X[y == label, 0].mean(),
              X[y == label, 1].mean() + 1.5,
              X[y == label, 2].mean(), species,
              bbox=dict(alpha=.5, edgecolor='g', facecolor='r'))

plot.scatter(X[:, 0], X[:, 1], X[:, 2], c=y)

plot.w_xaxis.set_ticklabels([])
plot.w_yaxis.set_ticklabels([])
plot.w_zaxis.set_ticklabels([])

plt.show()